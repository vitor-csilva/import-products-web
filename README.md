# DevOps Engineer Documentation

## About

The entire project was built using the concept of namespaces for organizational purposes and access control.

To assist in organizing the code and branches, the `Git-Flow` extension was used.

## Requirements for Using the Script

Below are all the necessary requirements for using the script:

1- Ensure all installations listed in the INSTALL.MD file are completed.

2- Install Make (a tool used to automate the process of building Docker images).

Follow the steps below to install it on Linux systems:
```
$ sudo apt-get update
$ sudo apt-get install make
```

## Dockerfile

The Dockerfile, located in the root of the project, was developed following best practices, prioritizing image size reduction and ensuring that layers are executed efficiently.

Although YAML format is highly readable for humans, the `Dockerfile.yaml` has been documented with comments explaining each step and its purpose.

## Development Environment

For the development environment, we use `Make`, a tool for automating processes that enhances productivity and efficiency. As a result, all image build, run, and compose build processes were executed using this tool.

### docker-compose.yaml

The `docker-compose.yaml` file, located in the project's root directory, was built with all necessary configurations, including environment variables, persistent data storage, proper port exposure, and startup order rules.

### Script Usage

Currently, there are three functionalities available in the script:

1- **build:**
   The `build` command constructs the image based on the instructions in `Dockerfile.yaml`, executes the `cron.sh` script to remove `.csv` files from the Backend's upload folder to reduce the image size, and specifically names the image `import-products`.
   Run the command from the project’s root directory:
   ```
   $ make build
   ```

2- **run:**
   The `run` command executes the `import-products` image generated by the build command.
   Run the command from the project’s root directory:
   ```
   $ make run
   ```

3- **compose:**
   The `compose` command is responsible for launching the entire project, ready for use. It builds the image from the instructions in `Dockerfile.yaml`, runs the `cron.sh` script to remove `.csv` files from the Backend's upload folder for a more optimized image, builds the image, and starts the generated image. This ensures that all necessary containers for running the application are available.
   Run the command from the project’s root directory:
   ```
   $ make compose
   ```

## Production Environment

For the production environment, the `pipeline.sh` script, located in the root directory, automates the build process, generates versions/tags, pushes the image to Docker, and updates the application’s deployment image in Kubernetes.

The `pipeline.sh` script includes several validations, such as checking whether an image version already exists on the host and verifying if an image with the same tag is already in our Docker Hub. This ensures no duplicate images and guarantees correct sequential versioning.

To execute `pipeline.sh`, run the following command from the project’s root directory:
```
$ ./pipeline.sh
```

We also use the `apply.sh` script, also located in the root directory. This script is responsible for creating the cluster, applying Kubernetes manifests, and triggering the `pipeline.sh` script, ensuring the entire environment is deployed with a single execution.

To execute `apply.sh`, run the following command from the project’s root directory:
```
$ ./apply.sh
```

### k8s

The tool used to simulate the Kubernetes environment was `kind`, where we created one control-plane and three data-planes for our cluster. All these definitions are in the `config.yaml` file, available in the `/k8s/manifests/config.yaml` directory.

All manifests used for deployment in the Kubernetes cluster can be accessed in the `/k8s` directory.

## Note

First of all, thank you for reading the documentation this far!

There are certainly areas for improvement in the project, especially regarding the automations I have implemented. Therefore, any feedback is valuable and important to help me improve my skills.

Thank you!!

Best regards,
Vitor Costa

